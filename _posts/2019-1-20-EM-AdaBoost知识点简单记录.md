---
layout:     post
title:      EM & AdaBoost 知识点简单记录
subtitle:   
date:       2019-1-20 00:00:00
author:     mzl
catalog:    true
tags:
    - EM
    - AdaBoost
---

{:toc}

# EM

EM 算法用于解决含有陷变量的概率模型参数的极大似然估计， 或极大后验概率估计。

EM 的应用主要为学习高斯混合模型等，EM 算法的推广为 GEM

EM 是通过迭代求解观测数据的对数极大似然估计 $L(\theta)=log P(Y|\theta)$ 来达到目标。
迭代过程主要分两步：1) E步，求期望(即 Q 函数)；2) M 步，求极大化

# AdaBoost

提升方法的思想：一个复杂任务，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

强可学习与弱可学习是等价的，即在概率近似正确的框架下，一个概念是强可学习的充要条件是这个概念是弱可学习的。这样一来，
可以先发现“弱学习算法”，然后提升为“强学习算法”即可，最常见的是 AdaBoost/GDBT 等。

提升方法有解决两个问题：
1. 在每一轮迭代中，如何改变训练数据的仅值或概率分布；
2. 如何将弱分类器组成成一个强分类器；

AdaBoost 将上面两个问题自然有效地实现在一个算法中。

AdaBoost 步骤：
1. 初始化时，所有训练数据权值均等
2. 迭代更新时，对 m 次迭代
    1. 根据权值计算当前迭代步骤的基本分类器，就是找到最合适的分割点将训练数据分为不同的类
    2. 在 1 的步骤上计算分类误差率
    3. 在 2 的步骤上计算当前分类器的系统
    4. 根据前面的步骤，更新训练数据的权值，这里误分类的数据仅值会增加，正确分类的数据权值会降低，从而提升分类准确率
3. 构建基本分类器的线性组合，得到最终的分类器

 AdaBoost 模型是加法模式、损失函数为指数函数、学习算法为向前分步算法的二分类学习方法。

## Boosting tree

Boosting tree 是以利用分类树或回归树为基本分类器的提升方法，它仍然是加法模型、前向分步算法，但基函数为决策树
看得出来，Boosting tree 可以用来解决回归问题了，但在解决回归问题时损失函数使用的是平方误差损失函数（解决分类问题
时的损失函数为指数损失函数）

当损失函数为非平方损失函数和非指数损失函数时，即为一般损失函数时，每一步优化比较困难，于是提升了梯度提升的算法，
就是利用最速下降法的近似方法，关键是利用损失函数的负梯度在当前模型中的值可靠优化，这就是 GDBT。

## XgBoost

XgBoost 对 GDBT 的改进，在于 GDBT 计算梯度时，只使用了一阶泰勒展开，而 XgBoost 考虑了二阶泰勒展开，并在目标函数中
加入了正则项以对整体求最优解。
